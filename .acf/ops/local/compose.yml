name: odp

# ODP Local Development Stack
# Profiles: minimal (Temporal only), dev (DEFAULT), full (all services)
# All ports: 15xxx-19xxx range (no conflicts with dev environments)

networks:
  odp-network:
    driver: bridge

volumes:
  postgres-data:
  redis-data:
  minio-data:
  neo4j-data:
  qdrant-data:
  temporal-data:
  dagster-data:

services:
  # ============================================================================
  # CORE INFRASTRUCTURE (minimal profile)
  # ============================================================================

  postgres:
    image: postgres:15-alpine
    profiles: ["minimal", "dev", "full"]
    container_name: odp-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: temporal
    ports:
      - "15432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/init-databases.sh:/docker-entrypoint-initdb.d/init-databases.sh:ro
    networks:
      - odp-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  temporal:
    image: temporalio/auto-setup:1.24.2
    profiles: ["minimal", "dev", "full"]
    container_name: odp-temporal
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=postgres
      - POSTGRES_PWD=postgres
      - POSTGRES_SEEDS=postgres
    ports:
      - "17233:7233"  # gRPC
      - "17234:7234"  # Membership
      - "17235:7235"  # History
    volumes:
      - temporal-data:/etc/temporal
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "workflow", "list"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 30s

  temporal-ui:
    image: temporalio/ui:2.28.0
    profiles: ["minimal", "dev", "full"]
    container_name: odp-temporal-ui
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:18088
    ports:
      - "18088:8080"
    networks:
      - odp-network

  # ============================================================================
  # EVENT BUS & CACHE (dev profile)
  # ============================================================================

  redis:
    image: redis:7-alpine
    profiles: ["dev", "full"]
    container_name: odp-redis
    command: redis-server --appendonly yes
    ports:
      - "16379:6379"
    volumes:
      - redis-data:/data
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # ============================================================================
  # OBJECT STORAGE (dev profile)
  # ============================================================================

  minio:
    image: minio/minio:latest
    profiles: ["dev", "full"]
    container_name: odp-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "19000:9000"  # API
      - "19001:9001"  # Console
    volumes:
      - minio-data:/data
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio-bootstrap:
    image: minio/mc:latest
    profiles: ["dev", "full"]
    container_name: odp-minio-bootstrap
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin;
      mc mb myminio/bronze --ignore-existing;
      mc mb myminio/silver --ignore-existing;
      mc mb myminio/gold --ignore-existing;
      mc mb myminio/mlflow --ignore-existing;
      echo 'MinIO buckets created: bronze, silver, gold, mlflow';
      exit 0;
      "
    networks:
      - odp-network

  # ============================================================================
  # BACKEND SERVICES (dev profile)
  # ============================================================================

  catalog-stub:
    build:
      context: ../../services/catalog-stub
      dockerfile: Dockerfile
    profiles: ["dev", "full"]
    container_name: odp-catalog-stub
    environment:
      - PORT=8080
      - CATALOG_FILE=/data/stub-catalog.json
    ports:
      - "18090:8080"
    volumes:
      - ../../data/catalog-metadata:/data
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  stubs:
    build:
      context: ../../services/stubs
      dockerfile: Dockerfile
    profiles: ["dev", "full"]
    container_name: odp-stubs
    environment:
      - PORT=8000
    ports:
      - "18086:8000"
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 10s
      timeout: 5s
      retries: 5

  user-api:
    build:
      context: ../../services/user-api
      dockerfile: Dockerfile
    profiles: ["dev", "full"]
    container_name: odp-user-api
    depends_on:
      redis:
        condition: service_healthy
      catalog-stub:
        condition: service_healthy
      temporal:
        condition: service_healthy
    environment:
      - PORT=8080
      - REDIS_URL=redis:6379
      - TEMPORAL_HOST=temporal:7233
      - CATALOG_URL=http://catalog-stub:8080
    ports:
      - "18080:8080"
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  yaml-processor:
    build:
      context: ../../services/yaml-processor
      dockerfile: Dockerfile
    profiles: ["dev", "full"]
    container_name: odp-yaml-processor
    depends_on:
      redis:
        condition: service_healthy
      temporal:
        condition: service_healthy
      catalog-stub:
        condition: service_healthy
    environment:
      - PORT=8080
      - REDIS_URL=redis:6379
      - TEMPORAL_HOST=temporal:7233
      - CATALOG_URL=http://catalog-stub:8080
    ports:
      - "18082:8080"
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  temporal-worker:
    build:
      context: ../../services/execution
      dockerfile: Dockerfile
    profiles: ["dev", "full"]
    container_name: odp-temporal-worker
    depends_on:
      temporal:
        condition: service_healthy
      redis:
        condition: service_healthy
      stubs:
        condition: service_healthy
    environment:
      - PYTHONUNBUFFERED=1
      - TEMPORAL_HOST=temporal:7233
      - REDIS_URL=redis:6379
      - STUBS_URL=http://stubs:8000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9000
      - AWS_REGION=us-east-1
      - BRONZE_PATH=s3a://bronze
      - SILVER_PATH=s3a://silver
      - GOLD_PATH=s3a://gold
    networks:
      - odp-network

  # ============================================================================
  # AI/ML SERVICES (full profile)
  # ============================================================================

  agent-orchestrator:
    build:
      context: ../../services/agent-orchestrator
      dockerfile: Dockerfile
    profiles: ["full"]
    container_name: odp-agent-orchestrator
    depends_on:
      qdrant:
        condition: service_healthy
      catalog-stub:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - PORT=8000
      - QDRANT_URL=http://qdrant:6333
      - CATALOG_URL=http://catalog-stub:8080
      - REDIS_URL=redis:6379
      - LLM_API_URL=${LLM_API_URL:-https://api.openai.com/v1}
      - LLM_API_KEY=${LLM_API_KEY:-}
    ports:
      - "18083:8000"
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  qdrant:
    image: qdrant/qdrant:v1.15.5
    profiles: ["full"]
    container_name: odp-qdrant
    ports:
      - "16333:6333"  # HTTP
      - "16334:6334"  # gRPC
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - odp-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/localhost/6333'"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ============================================================================
  # DATA PLATFORM (full profile)
  # ============================================================================

  dagster:
    build:
      context: ../../data/dagster
      dockerfile: Dockerfile
    profiles: ["full"]
    container_name: odp-dagster
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - DAGSTER_POSTGRES_USER=postgres
      - DAGSTER_POSTGRES_PASSWORD=postgres
      - DAGSTER_POSTGRES_DB=dagster
      - DAGSTER_POSTGRES_HOST=postgres
      - DAGSTER_POSTGRES_PORT=5432
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9000
      - AWS_REGION=us-east-1
      - REDIS_URL=redis:6379
    ports:
      - "18084:3000"  # Web UI
      - "18085:4000"  # gRPC
    volumes:
      - dagster-data:/opt/dagster/app/storage
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:3000')"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 90s

  mlflow:
    build:
      context: mlflow
      dockerfile: Dockerfile
    profiles: ["full"]
    container_name: odp-mlflow
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    command: >
      mlflow server
      --backend-store-uri postgresql://postgres:postgres@postgres:5432/mlflow
      --default-artifact-root s3://mlflow/
      --host 0.0.0.0
      --port 5000
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
    ports:
      - "18087:5000"
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # GRAPH DATABASE (full profile)
  # ============================================================================

  neo4j:
    image: neo4j:5.13-community
    profiles: ["full"]
    container_name: odp-neo4j
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_dbms_memory_pagecache_size=512M
      - NEO4J_dbms_memory_heap_max__size=1G
    ports:
      - "17687:7687"  # Bolt
      - "17474:7474"  # HTTP
    volumes:
      - neo4j-data:/data
    networks:
      - odp-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:7474"]
      interval: 10s
      timeout: 5s
      retries: 5
